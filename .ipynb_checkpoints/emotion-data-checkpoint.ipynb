{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170e3545-28eb-494f-9fde-6029c9c2a6e1",
   "metadata": {},
   "source": [
    "# GoEmotions data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73e8293-bf46-4f48-9123-f3f126e2560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elmaneva/mambaforge/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████████████| 9.40k/9.40k [00:00<00:00, 4.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/raw to /home/elmaneva/.cache/huggingface/datasets/parquet/raw-9773852f9507dadf/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/24.8M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                     | 42.0k/24.8M [00:00<01:08, 360kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 112k/24.8M [00:00<00:45, 540kB/s]\u001b[A\n",
      "Downloading data:   1%|▏                     | 225k/24.8M [00:00<00:35, 695kB/s]\u001b[A\n",
      "Downloading data:   2%|▍                    | 511k/24.8M [00:00<00:17, 1.38MB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 913k/24.8M [00:00<00:10, 2.20MB/s]\u001b[A\n",
      "Downloading data:   6%|█▏                  | 1.48M/24.8M [00:00<00:07, 3.20MB/s]\u001b[A\n",
      "Downloading data:   9%|█▋                  | 2.12M/24.8M [00:00<00:05, 3.91MB/s]\u001b[A\n",
      "Downloading data:  12%|██▎                 | 2.94M/24.8M [00:00<00:04, 5.16MB/s]\u001b[A\n",
      "Downloading data:  14%|██▊                 | 3.47M/24.8M [00:01<00:04, 4.74MB/s]\u001b[A\n",
      "Downloading data:  17%|███▍                | 4.30M/24.8M [00:01<00:03, 5.70MB/s]\u001b[A\n",
      "Downloading data:  20%|███▉                | 4.94M/24.8M [00:01<00:03, 5.89MB/s]\u001b[A\n",
      "Downloading data:  22%|████▍               | 5.54M/24.8M [00:01<00:03, 5.32MB/s]\u001b[A\n",
      "Downloading data:  25%|████▉               | 6.10M/24.8M [00:01<00:03, 5.30MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▎              | 6.64M/24.8M [00:01<00:03, 5.20MB/s]\u001b[A\n",
      "Downloading data:  29%|█████▊              | 7.17M/24.8M [00:01<00:03, 4.53MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▏             | 7.73M/24.8M [00:01<00:03, 4.79MB/s]\u001b[A\n",
      "Downloading data:  33%|██████▋             | 8.26M/24.8M [00:01<00:03, 4.90MB/s]\u001b[A\n",
      "Downloading data:  36%|███████             | 8.82M/24.8M [00:02<00:03, 5.06MB/s]\u001b[A\n",
      "Downloading data:  38%|███████▋            | 9.53M/24.8M [00:02<00:02, 5.62MB/s]\u001b[A\n",
      "Downloading data:  42%|████████▎           | 10.4M/24.8M [00:02<00:02, 6.33MB/s]\u001b[A\n",
      "Downloading data:  44%|████████▊           | 11.0M/24.8M [00:02<00:02, 6.15MB/s]\u001b[A\n",
      "Downloading data:  47%|█████████▍          | 11.7M/24.8M [00:02<00:02, 6.12MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████          | 12.4M/24.8M [00:02<00:02, 6.18MB/s]\u001b[A\n",
      "Downloading data:  54%|██████████▋         | 13.3M/24.8M [00:02<00:01, 6.89MB/s]\u001b[A\n",
      "Downloading data:  57%|███████████▎        | 14.0M/24.8M [00:02<00:01, 6.40MB/s]\u001b[A\n",
      "Downloading data:  59%|███████████▊        | 14.7M/24.8M [00:03<00:02, 3.47MB/s]\u001b[A\n",
      "Downloading data:  64%|████████████▊       | 15.9M/24.8M [00:03<00:01, 4.84MB/s]\u001b[A\n",
      "Downloading data:  67%|█████████████▍      | 16.7M/24.8M [00:03<00:01, 5.23MB/s]\u001b[A\n",
      "Downloading data:  70%|█████████████▉      | 17.3M/24.8M [00:03<00:01, 5.53MB/s]\u001b[A\n",
      "Downloading data:  72%|██████████████▍     | 18.0M/24.8M [00:03<00:01, 5.71MB/s]\u001b[A\n",
      "Downloading data:  75%|███████████████     | 18.7M/24.8M [00:03<00:01, 5.75MB/s]\u001b[A\n",
      "Downloading data:  79%|███████████████▊    | 19.6M/24.8M [00:03<00:00, 5.99MB/s]\u001b[A\n",
      "Downloading data:  82%|████████████████▎   | 20.3M/24.8M [00:04<00:00, 6.03MB/s]\u001b[A\n",
      "Downloading data:  84%|████████████████▊   | 20.9M/24.8M [00:04<00:00, 5.30MB/s]\u001b[A\n",
      "Downloading data:  87%|█████████████████▍  | 21.6M/24.8M [00:04<00:00, 5.77MB/s]\u001b[A\n",
      "Downloading data:  90%|██████████████████  | 22.4M/24.8M [00:04<00:00, 6.27MB/s]\u001b[A\n",
      "Downloading data:  93%|██████████████████▌ | 23.1M/24.8M [00:04<00:00, 6.06MB/s]\u001b[A\n",
      "Downloading data:  95%|███████████████████ | 23.7M/24.8M [00:04<00:00, 6.06MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 24.8M/24.8M [00:04<00:00, 5.18MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/2.77M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▍                    | 65.5k/2.77M [00:00<00:04, 617kB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 228k/2.77M [00:00<00:02, 1.16MB/s]\u001b[A\n",
      "Downloading data:  18%|███▋                 | 490k/2.77M [00:00<00:01, 1.75MB/s]\u001b[A\n",
      "Downloading data:  39%|███████▊            | 1.08M/2.77M [00:00<00:00, 3.28MB/s]\u001b[A\n",
      "Downloading data:  61%|████████████▏       | 1.68M/2.77M [00:00<00:00, 4.09MB/s]\u001b[A\n",
      "Downloading data:  78%|███████████████▌    | 2.16M/2.77M [00:00<00:00, 4.24MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 2.77M/2.77M [00:00<00:00, 3.65MB/s]\u001b[A\n",
      "Downloading data files:  33%|███████              | 1/3 [00:09<00:19,  9.68s/it]\n",
      "Downloading data:   0%|                              | 0.00/350k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   9%|██                    | 32.8k/350k [00:00<00:01, 285kB/s]\u001b[A\n",
      "Downloading data:  33%|███████▌               | 115k/350k [00:00<00:00, 577kB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 350k/350k [00:00<00:00, 1.04MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████████████       | 2/3 [00:11<00:04,  4.82s/it]\n",
      "Downloading data:   0%|                              | 0.00/347k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|██▏                   | 34.8k/347k [00:00<00:00, 348kB/s]\u001b[A\n",
      "Downloading data:  52%|███████████▉           | 179k/347k [00:00<00:00, 908kB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 347k/347k [00:00<00:00, 1.01MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 3/3 [00:12<00:00,  4.28s/it]\n",
      "Extracting data files: 100%|█████████████████████| 3/3 [00:00<00:00, 606.32it/s]\n",
      "Generating train split:  95%|▉| 200000/211225 [00:00<00:00, 909906.39 examples/sFailed to read file '/home/elmaneva/.cache/huggingface/datasets/downloads/81930a619f92e4ed014565eb16346f9daac14cb953813b6ec288f091d30f4d41' with error <class 'ValueError'>: Couldn't cast\n",
      "text: string\n",
      "labels: list<item: int64>\n",
      "  child 0, item: int64\n",
      "id: string\n",
      "-- schema metadata --\n",
      "huggingface: '{\"info\": {\"features\": {\"text\": {\"dtype\": \"string\", \"_type\":' + 470\n",
      "to\n",
      "{'text': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'subreddit': Value(dtype='string', id=None), 'link_id': Value(dtype='string', id=None), 'parent_id': Value(dtype='string', id=None), 'created_utc': Value(dtype='float32', id=None), 'rater_id': Value(dtype='int32', id=None), 'example_very_unclear': Value(dtype='bool', id=None), 'admiration': Value(dtype='int32', id=None), 'amusement': Value(dtype='int32', id=None), 'anger': Value(dtype='int32', id=None), 'annoyance': Value(dtype='int32', id=None), 'approval': Value(dtype='int32', id=None), 'caring': Value(dtype='int32', id=None), 'confusion': Value(dtype='int32', id=None), 'curiosity': Value(dtype='int32', id=None), 'desire': Value(dtype='int32', id=None), 'disappointment': Value(dtype='int32', id=None), 'disapproval': Value(dtype='int32', id=None), 'disgust': Value(dtype='int32', id=None), 'embarrassment': Value(dtype='int32', id=None), 'excitement': Value(dtype='int32', id=None), 'fear': Value(dtype='int32', id=None), 'gratitude': Value(dtype='int32', id=None), 'grief': Value(dtype='int32', id=None), 'joy': Value(dtype='int32', id=None), 'love': Value(dtype='int32', id=None), 'nervousness': Value(dtype='int32', id=None), 'optimism': Value(dtype='int32', id=None), 'pride': Value(dtype='int32', id=None), 'realization': Value(dtype='int32', id=None), 'relief': Value(dtype='int32', id=None), 'remorse': Value(dtype='int32', id=None), 'sadness': Value(dtype='int32', id=None), 'surprise': Value(dtype='int32', id=None), 'neutral': Value(dtype='int32', id=None)}\n",
      "because column names don't match\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/builder.py:1879\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1878\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1879\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:82\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cast_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:61\u001b[0m, in \u001b[0;36mParquet._cast_table\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# more expensive cast to support nested features with keys in a different order\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# allows str <-> int/float or str to Audio for example\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/table.py:2324\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/table.py:2282\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumn_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28msorted\u001b[39m(features):\n\u001b[0;32m-> 2282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2283\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't cast\ntext: string\nlabels: list<item: int64>\n  child 0, item: int64\nid: string\n-- schema metadata --\nhuggingface: '{\"info\": {\"features\": {\"text\": {\"dtype\": \"string\", \"_type\":' + 470\nto\n{'text': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'author': Value(dtype='string', id=None), 'subreddit': Value(dtype='string', id=None), 'link_id': Value(dtype='string', id=None), 'parent_id': Value(dtype='string', id=None), 'created_utc': Value(dtype='float32', id=None), 'rater_id': Value(dtype='int32', id=None), 'example_very_unclear': Value(dtype='bool', id=None), 'admiration': Value(dtype='int32', id=None), 'amusement': Value(dtype='int32', id=None), 'anger': Value(dtype='int32', id=None), 'annoyance': Value(dtype='int32', id=None), 'approval': Value(dtype='int32', id=None), 'caring': Value(dtype='int32', id=None), 'confusion': Value(dtype='int32', id=None), 'curiosity': Value(dtype='int32', id=None), 'desire': Value(dtype='int32', id=None), 'disappointment': Value(dtype='int32', id=None), 'disapproval': Value(dtype='int32', id=None), 'disgust': Value(dtype='int32', id=None), 'embarrassment': Value(dtype='int32', id=None), 'excitement': Value(dtype='int32', id=None), 'fear': Value(dtype='int32', id=None), 'gratitude': Value(dtype='int32', id=None), 'grief': Value(dtype='int32', id=None), 'joy': Value(dtype='int32', id=None), 'love': Value(dtype='int32', id=None), 'nervousness': Value(dtype='int32', id=None), 'optimism': Value(dtype='int32', id=None), 'pride': Value(dtype='int32', id=None), 'realization': Value(dtype='int32', id=None), 'relief': Value(dtype='int32', id=None), 'remorse': Value(dtype='int32', id=None), 'sadness': Value(dtype='int32', id=None), 'surprise': Value(dtype='int32', id=None), 'neutral': Value(dtype='int32', id=None)}\nbecause column names don't match",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgo_emotions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dataset\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1797\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1808\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1809\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/builder.py:909\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/builder.py:1004\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1011\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/builder.py:1767\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1765\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1767\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1768\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1769\u001b[0m     ):\n\u001b[1;32m   1770\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1771\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/datasets/builder.py:1912\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1911\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m-> 1912\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c5efa-db90-43ca-8377-ccc50f8ec2e5",
   "metadata": {},
   "source": [
    "# XED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3172a870-acc4-432b-a44b-1a824a965d78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# get all files per country\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os \n",
    "# get all files per country\n",
    "path = \"emotion-data/XED\"\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a0369b-56eb-4c0e-abeb-7e34ccc4f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame()\n",
    "schema = {\"text\":pl.String,\n",
    "          \"labels\":pl.String,\n",
    "         \"language\":pl.String}\n",
    "for f in files:\n",
    "    tmp = pl.read_csv(path+\"/\"+f, separator=\"\\t\",schema=schema, ignore_errors=True)\n",
    "    lang = f.split(\"-\")[0]\n",
    "    tmp = tmp.with_columns(language=pl.Series([lang]*len(tmp)))\n",
    "    if tmp.is_empty():\n",
    "        df=tmp\n",
    "    else:\n",
    "        df=pl.concat([df,tmp],how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17daabb-c6a1-4a4f-a94d-ab2e4e61db78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (207_909, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th><th>labels</th><th>language</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Ruumiita ripus…</td><td>&quot;1, 3, 4&quot;</td><td>&quot;fi&quot;</td></tr><tr><td>&quot;Ei mitään mutt…</td><td>&quot;1&quot;</td><td>&quot;fi&quot;</td></tr><tr><td>&quot;Älä anna hänen…</td><td>&quot;1&quot;</td><td>&quot;fi&quot;</td></tr><tr><td>&quot;Laske aseet ma…</td><td>&quot;1, 4&quot;</td><td>&quot;fi&quot;</td></tr><tr><td>&quot;Vittuun toimis…</td><td>&quot;1&quot;</td><td>&quot;fi&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;美國人民 我聽到了你們的聲音…</td><td>&quot;8&quot;</td><td>&quot;zh&quot;</td></tr><tr><td>&quot;她很擅長她的工作&quot;</td><td>&quot;1, 3&quot;</td><td>&quot;zh&quot;</td></tr><tr><td>&quot;湯姆...&quot;</td><td>&quot;8&quot;</td><td>&quot;zh&quot;</td></tr><tr><td>&quot;如果他們要和我見面怎么辦&quot;</td><td>&quot;2, 5&quot;</td><td>&quot;zh&quot;</td></tr><tr><td>&quot;我沒有朋友&quot;</td><td>&quot;2, 5&quot;</td><td>&quot;zh&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (207_909, 3)\n",
       "┌───────────────────────────────────┬─────────┬──────────┐\n",
       "│ text                              ┆ labels  ┆ language │\n",
       "│ ---                               ┆ ---     ┆ ---      │\n",
       "│ str                               ┆ str     ┆ str      │\n",
       "╞═══════════════════════════════════╪═════════╪══════════╡\n",
       "│ Ruumiita ripustettuina - ruumiit… ┆ 1, 3, 4 ┆ fi       │\n",
       "│ Ei mitään muttia.                 ┆ 1       ┆ fi       │\n",
       "│ Älä anna hänen määräillä sinua!   ┆ 1       ┆ fi       │\n",
       "│ Laske aseet maahan.               ┆ 1, 4    ┆ fi       │\n",
       "│ Vittuun toimisto!                 ┆ 1       ┆ fi       │\n",
       "│ …                                 ┆ …       ┆ …        │\n",
       "│ 美國人民 我聽到了你們的聲音\n",
       "       ┆ 8       ┆ zh       │\n",
       "│ da/2017/6411842/…                 ┆         ┆          │\n",
       "│ 她很擅長她的工作                  ┆ 1, 3    ┆ zh       │\n",
       "│ 湯姆...                           ┆ 8       ┆ zh       │\n",
       "│ 如果他們要和我見面怎么辦          ┆ 2, 5    ┆ zh       │\n",
       "│ 我沒有朋友                        ┆ 2, 5    ┆ zh       │\n",
       "└───────────────────────────────────┴─────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6240f776-457f-400c-af56-192e791e77b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (318,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>labels</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;1, 2, 3, 4, 6&quot;</td></tr><tr><td>&quot;8, 2, 4&quot;</td></tr><tr><td>&quot;6, 7, 1&quot;</td></tr><tr><td>&quot;4, 5, 7&quot;</td></tr><tr><td>&quot;1, 7, 8&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;2, 4, 5, 7, 8&quot;</td></tr><tr><td>&quot;8, 2, 3, 6&quot;</td></tr><tr><td>&quot;2, 3, 4, 6, 7&quot;</td></tr><tr><td>&quot;2, 4, 6&quot;</td></tr><tr><td>&quot;2, 5, 8&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (318,)\n",
       "Series: 'labels' [str]\n",
       "[\n",
       "\t\"1, 2, 3, 4, 6\"\n",
       "\t\"8, 2, 4\"\n",
       "\t\"6, 7, 1\"\n",
       "\t\"4, 5, 7\"\n",
       "\t\"1, 7, 8\"\n",
       "\t…\n",
       "\t\"2, 4, 5, 7, 8\"\n",
       "\t\"8, 2, 3, 6\"\n",
       "\t\"2, 3, 4, 6, 7\"\n",
       "\t\"2, 4, 6\"\n",
       "\t\"2, 5, 8\"\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f906bd32-510f-41f3-9572-ec59ed5b1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude languages not present in parlamint 4.0 dataset\n",
    "# language codes by ISO 639 in parlamint\n",
    "lang_codes = [\"bs\",\"bg\",\"hr\",\"cs\",\"da\",\"nl\",\"en\",\"et\",\"fi\",\"fr\",\"de\",\n",
    "                 \"hu\",\"is\",\"it\",\"lv\",\"el\",\"no\",\"pl\",\"pt\",\"ru\",\"sr\",\"sl\",\n",
    "                 \"es\",\"sv\",\"tr\",\"uk\"]\n",
    "\n",
    "# exclude langs\n",
    "df = df.filter(pl.col(\"language\").is_in(lang_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70947685-5eb4-4d0e-8d9e-0e81218270f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string labels to list and recode from 0 to n-1\n",
    "df = df.with_columns(pl.col(\"labels\").map_elements(lambda s: [[int(x)-1] for x in s.split(\",\") if x.isdigit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae5bfa-6687-4c05-bf76-e511e0c83b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess class-imbalance problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b3e625d-c325-406a-aaa6-938fb2945e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full dataset\n",
    "df.select(pl.col(\"text\",\"labels\")).write_parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "01b541e3-c1f7-4658-b4bf-b395524ac64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (207_909,)\n",
      "Series: 'labels' [list[list[i64]]]\n",
      "[\n",
      "\t[[0], [2], [3]]\n",
      "\t[[0]]\n",
      "\t[[0]]\n",
      "\t[[0], [3]]\n",
      "\t[[0]]\n",
      "\t…\n",
      "\t[[7]]\n",
      "\t[[0], [2]]\n",
      "\t[[7]]\n",
      "\t[[1], [4]]\n",
      "\t[[1], [4]]\n",
      "]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#np.unique(y)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_sample_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/5bv2nm3c478ppikbh49cp5436wy3k1ii-python3.11-scikit-learn-1.4.1.post1/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/nix/store/5bv2nm3c478ppikbh49cp5436wy3k1ii-python3.11-scikit-learn-1.4.1.post1/lib/python3.11/site-packages/sklearn/utils/class_weight.py:185\u001b[0m, in \u001b[0;36mcompute_sample_weight\u001b[0;34m(class_weight, y, indices)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     y_full \u001b[38;5;241m=\u001b[39m y[:, k]\n\u001b[0;32m--> 185\u001b[0m classes_full \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m classes_missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_outputs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/nix/store/x1rqfn240xn6m6p6077gxfqxdxxj1cmc-python3.11-numpy-1.26.4/lib/python3.11/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/x1rqfn240xn6m6p6077gxfqxdxxj1cmc-python3.11-numpy-1.26.4/lib/python3.11/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (3,) "
     ]
    }
   ],
   "source": [
    "# calculate class-weights\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "y = df[\"labels\"]\n",
    "print(y)\n",
    "#np.unique(y)\n",
    "class_weights = compute_sample_weight(class_weight=\"balanced\", y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106a7e3-056d-49e0-9df2-0a22d7a789fd",
   "metadata": {},
   "source": [
    "# Combining datasets ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cee6a43-a250-495a-a9dd-a98e8e6574f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode labels goem\n",
    "labels_goem = {'0': \"admiration\",\n",
    "              '1': \"amusement\",\n",
    "              '2': \"anger\",\n",
    "              '3': \"annoyance\",\n",
    "              '4': \"approval\",\n",
    "              '5': \"caring\",\n",
    "              '6': \"confusion\",\n",
    "              '7': \"curiosity\",\n",
    "              '8': \"desire\",\n",
    "              '9': \"disappointment\",\n",
    "              '10': \"disapproval\",\n",
    "              '11': \"disgust\",\n",
    "              '12': \"embarrassment\", \n",
    "              '13': \"excitement\",\n",
    "              '14': \"fear\",\n",
    "              '15': \"gratitude\",\n",
    "              '16': \"grief\",\n",
    "              '17': \"joy\",\n",
    "              '18': \"love\",\n",
    "              '19': \"nervousness\",\n",
    "              '20': \"optimism\",\n",
    "              '21': \"pride\",\n",
    "              '22': \"realization\",\n",
    "              '23': \"relief\",\n",
    "              '24': \"remorse\",\n",
    "              '25': \"sadness\",\n",
    "              '26': \"surprise\",\n",
    "              '27': \"neutral\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64d41fe0-5a99-476f-9bff-ec9742ef9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2labels XED\n",
    "id2labels = {'1':\"anger\", \n",
    "            '2':\"anticipation\",\n",
    "              '3':\"disgust\", \n",
    "              '4':\"fear\", \n",
    "              '5':\"joy\", \n",
    "              '6':\"sadness\", \n",
    "              '7':\"surprise\", \n",
    "              '8':\"trust\",\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9f29cf9-b918-4803-ada9-709f0bdd5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change order key value to recode through retrieval\n",
    "labels2id = {v:k for k,v in labels_xed.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c893b-6b9e-416e-bc22-1be22507a0a3",
   "metadata": {},
   "source": [
    "# Model training notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71b74f-8d3b-405a-9097-f241e26946f4",
   "metadata": {},
   "source": [
    "- Train two different models: one multi- and one single-label\n",
    "- Train one model on only english data and one on translated data\n",
    "- Train one model on a combination of XED and GoEmotions data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
